---
title: "Final Presentation"
author: "Heather Richard, Boo Richard, and Beth Davis"
format: revealjs
editor: visual
---

```{r}
library(RCurl)
library(ggplot2)
library(hillR)
library(reshape2)
library(dplyr)
library(MASS)
library(fitdistrplus)
```

# Project Overview

# What was your question?

-   We used data from an experiment done in 2015 in the San Francisco Bay looking at metal accumulation (ng/mL) with biofilm growth over a span of 28 days at California Maritime Academy (CMA) and Estuary and Ocean Science Center (EOS) on three different pelleted materials: low-density polyethylene (LDPE), polylactic acid (PLA) and glass.

    ![](images/richard2019.jpg)

    ![](images/paste-49617460.png)

    Graphical abstract from Richard et al., 2019

# We wanted to know:

-   What distribution function best fit the data from this experiment, and what might it indicate about the chemical and biological processes at work

-   What other ways might we use theoretical modeling to better understand the data and vice versa?

## How did you try to approach it?

We will need to approach this project as though different metal types are different species. We start by loading the libraries we will need and the data we are using.

```{r}
x <- getURL("https://raw.githubusercontent.com/HLR-04614/final-project/main/metalsdata.csv")
data <- read.csv(text = x)

head(data)
```

We need to format it so it will work with some of the code we tried in class. We use optical density as a proxy for biofilm and keep it as an ID variable for later analysis.

```{r}
data1<-melt(data, id = c("Number","Sample","Weight","Unbrushed", "Location","Material","Day.deploy","Optical.density","Replicate")) 

```

Then group the data and average into a column called "mean_replicate"

```{r}
data2 <- data1 %>% group_by(variable, Location, Day.deploy, Material) %>% summarize(mean_replicate=mean(value), stdev=sd(value))

head(data2)
```

Then add a column for rank so we can work with Hill Numbers later.

```{r}
data3<-data2 %>% arrange(variable, mean_replicate) %>%
    group_by(Location, Material, Day.deploy) %>% 
    mutate(rank = rank(mean_replicate))
head(data3)
```

Now we can create an exploratory graph based on material and date of deployment

```{r Data}
ggplot(data3, aes(rank, mean_replicate, color=Day.deploy)) +
  geom_point() +
  scale_y_log10() +
  scale_colour_gradient() +
  facet_grid(Location~Material) +
  theme_bw()
```

We start by running the code from "[Notes on likelihood](https://github.com/eco-evo-thr-2022/10-likelihood/blob/main/likelihood_how-to.Rmd)" from lab 10. To make the data look like SADs, we need to sort in decreasing rank abundance and subset the data. Variables for different sampling times are still within each subset.

```{r warning=FALSE}
CMA<-subset(data3, Location=="CM")
EOS<-subset(data3, data3$Location=="RTC")
EOS_PLA<-subset(EOS, EOS$Material=="PLA")
EOS_Glass<-subset(EOS, EOS$Material=="Glass")
EOS_LDPE<-subset(EOS, EOS$Material=="LDPE")
CMA_PLA<-subset(CMA, CMA$Material=="PLA")
CMA_Glass<-subset(CMA, CMA$Material=="Glass")
CMA_LDPE<-subset(CMA, CMA$Material=="LDPE")


par(mfrow=c(2,3))
plot(sort(EOS_LDPE$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "EOS LDPE") 

plot(sort(EOS_PLA$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "EOS PLA") 

plot(sort(EOS_Glass$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "EOS Glass") 

plot(sort(CMA_LDPE$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "CMA LDPE") 

plot(sort(CMA_PLA$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "CMA PLA") 

plot(sort(CMA_Glass$mean_replicate, decreasing = TRUE), log = 'y', xlab = 'Species rank', ylab = 'Abundance', main= "CMA Glass") 
```

These plots look pretty similar, so for simplicity sake maybe we can analyze them all together for now.

Does the log series equation describe our data well? That could indicate neutral processes.

b= beta. \<-What is this?

n= species abundance

Species abundance data in the code is 1:50, but that's not quite going to work with our data since our metal "abundance" values are smaller continuous numbers.

```{r Histogram }
lseries <- function(b, n) {
   1/log(1 / (1 - exp(-b))) * exp(-b * n) / n}

#lseries(0.001, 0.0001:100)

hist(data3$mean_replicate, probability = TRUE)

points(0.0001:100, lseries(0.001, 0.0001:100), col = 'red', type = 'b')
points(0.0001:100, lseries(0.01, 0.0001:100), col = 'orange', type = 'b')
points(0.0001:100, lseries(0.1, 0.0001:100), col = 'blue', type = 'b')
```

```{r}
#lseries(0.01, data3$mean_replicate) #It's a long list with inf values so this will have inf values too
prod(lseries(0.01, data3$mean_replicate))
prod(lseries(0.5, data3$mean_replicate))  
```

The infinite values from the 0's in our data are a problem. Here is a messy workaround, but for more discussion see [this article.](https://www.researchgate.net/post/Log_transformation_of_values_that_include_0_zero_for_statistical_analyses2)

```{r include=TRUE}
data3$mean2<-data3$mean_replicate+0.000000001

sum(log(lseries(0.1, data3$mean2)))
sum(log(lseries(0.01, data3$mean2)))
sum(log(lseries(0.001, data3$mean2)))
```

Now we look at the max likelihood to get the max likelihood estimate of our logseries distribution. What's the best beta value?

```{r echo=FALSE}
bb <- seq(0.001, 0.2, length.out = 50)
ll <- sapply(bb, function(b) sum(log(lseries(b, data3$mean2))))
plot(bb, ll, xlab = 'b parameter', ylab = 'log likelihood')
```

Looks to be around 0.06. Let's optimize to find out the exact value:

```{r}
llLSeries <- function(b, n) {
    sum(log(lseries(b, n)))
}
optimize(llLSeries, interval = c(0, 10), n = data3$mean2, maximum = TRUE)
# This may show an 'NA/Inf replaced by maximum positive value' error
```

#### What did you find? OR, what obstacles did you encounter?

We had a hard time with our data because of the 0 values and the non-integer nature of our metal amounts. For this reason, we chose to use the fitdistr function from the MASS package and fitdist from the fitdistrplus package to help us figure out distributions that might better describe our data, following [this tutorial](http://www.di.fc.ul.pt/~jpn/r/distributions/fitting.html) with [this document](https://cran.r-project.org/web/packages/fitdistrplus/vignettes/paper2JSS.pdf) for additional background information.

```{r echo=FALSE, warning=FALSE}
fit <- fitdistr(data3$mean2, densfun="normal")  
fit1 <- fitdistr(data3$mean2, densfun="lognormal")  
fit2 <- fitdistr(data3$mean2, densfun="gamma")
fit3 <- fitdistr(data3$mean2, densfun="poisson")  
fit4 <- fitdistr(data3$mean2, densfun="weibull")

hist(data3$mean2, pch=20, breaks=25, prob=TRUE, main="")
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col="red", lwd=2, add=T)
curve(dnorm(x, fit1$estimate[1], fit1$estimate[2]), col="blue", lwd=2, add=T)
curve(dnorm(x, fit2$estimate[1], fit2$estimate[2]), col="yellow", lwd=2, add=T)
curve(dnorm(x, fit3$estimate[1], fit3$estimate[2]), col="orange", lwd=2, add=T)
curve(dnorm(x, fit4$estimate[1], fit4$estimate[2]), col="green", lwd=2, add=T)
```

It looks like the log normal (blue) distribution and the weibull (green) distribution might be the best fit for our data.

```{r}
log_likelihood <- function(params) { -sum(dnorm(data3$mean2, params[1], params[2], log=TRUE)) }
fitll <- optim(c(0,1), log_likelihood)    
fitll
```

#### What does this mean?

Fitdistrplus can make this process a bit easier with neat descriptive stats:

```{r}
plotdist(data3$mean2, histo = TRUE, demp = TRUE)
```

descdist

```{r}
descdist(data3$mean2, discrete=FALSE, boot=500)
```

```{r}
fit_w  <- fitdist(data3$mean2, "weibull")
fit_ln <- fitdist(data3$mean2, "lnorm")
fit_g  <- fitdist(data3$mean2, "gamma")

summary(fit_w)
summary(fit_ln)
summary(fit_g)
```

CDFs

```{r echo=FALSE}
par(mfrow=c(2,2))
plot.legend <- c("Weibull", "lognormal", "gamma")
denscomp(list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
cdfcomp (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
qqcomp  (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
ppcomp  (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)

```

cdfcomp

```{r}
cdfcomp(list(fit_ln, fit_w, fit_g), xlogscale = TRUE, ylogscale = TRUE,
        legendtext = c("lognormal", "weibull", "gamma"))
```

Goodness of fit stats

```{r include=TRUE}
gofstat(list(fit_ln, fit_w, fit_g), fitnames = c("lnorm", "weibull", "gamma"))
```

```{r include=TRUE}
ests <- bootdist(fit_g, niter = 1e3)
summary(ests)
# ^ This may take some time to run, don't interrupt it!
```

```{r}
plot(ests)
```

```{r}
quantile(ests, probs=.05) 
```

#### From Delignette-Muller, 2014 (updated 2020)

"In ecotoxicology, a lognormal or a loglogistic distribution is often fitted to such a data set in order to characterize the species sensitivity distribution (SSD) for a pollutant. A low percentile of the fitted distribution, generally the 5% percentile, is then calculated and named the hazardous concentration 5% (HC5). It is interpreted as the value of the pollutant concentration protecting 95% of the species (Posthuma et al., 2010)." The above quantile code was made to describe the HC5 of a dataset of the acute toxicity of endosulfan for different species described best by the Burr distribution.

## Hill Numbers

We also went through the process of calculating Hill Numbers for our species data (with 0's unaltered) and was able to reproduce the number using the hill_taxa function

```{r echo=TRUE}
metals_sad<-data3$mean_replicate
q<- 2

hillR::hill_taxa(metals_sad, q)
```

So is there a difference in metal richness comparing the different locations and materials used in the experiment?

```{r echo=TRUE}
ELDPE <- hill_taxa(EOS_LDPE$mean_replicate, 0) # Result: 99
CLDPE <- hill_taxa(CMA_LDPE$mean_replicate, 0) # Result: 78
```

```{r echo=TRUE}
EGlass <- hill_taxa(EOS_Glass$mean_replicate, 0) # Result: 98
CGlass <- hill_taxa(CMA_Glass$mean_replicate, 0) # Result: 81
```

```{r echo=TRUE}
EPLA <- hill_taxa(EOS_PLA$mean_replicate, 0) # Result: 93
CPLA <- hill_taxa(CMA_PLA$mean_replicate, 0) # Result: 69
```

The metal richness is consistently higher at the Estuary and Ocean Science Center.

```{r echo=TRUE}
hilltaxa <- matrix(c(ELDPE, CLDPE, EGlass, CGlass, EPLA, CPLA), ncol = 2, byrow = TRUE)
colnames(hilltaxa) <- c('Estuary Metal Richness', 'OSC Metal Richness')
rownames(hilltaxa) <- c('LDPE', 'Glass', 'PLA')

hilltaxa
```

It seems that the differences among material types are not as big as the differences between locations (99, 93 and 98 for the materials at EOS vs 78, 69, and 81 for the materials at CMA). Neat!

## What do you think are the next steps?

Looking at more Hill numbers and using different values of q, we can keep asking questions that compare various biodiversity metrics among the groups. We can make a column in our data that has the different metrics at each time points for each sample and then look at patterns in metal accumulation over time, comparing richness with evenness and the rate of accumulation. It'd be neat to look at distribution changes over time, to see how the predictive models change as biofilm grows and correlate the biodiversity metrics with biofilm quantity as well as age.

We might also ask how metal type influences accumulation patterns- that would be creating a column with the metal family associated with each metal and see if data patterns show similarity by family.

## What would be your words of wisdom to someone trying to continue this work?

There are a ton of resources out there, and it was helpful to explore those resources when we hit a wall or were unsure where to go next with the analysis. It seemed difficult at first, because we expected this to be a rather unconventional dataset for they type of analysis, but it turns out there are accepted data analysis parallels in the world of ecotoxicology. Who knew?!

## What would you like to take away/remember most about the final project experience?

Github and R Markdown are really neat ways of sharing and presenting data projects. And even if data analysis doesn't turn out the way you expected it would, there is a lot to be learned by just playing around and asking questions.
