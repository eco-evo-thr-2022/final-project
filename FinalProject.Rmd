---
title: "BIO 597 Final Project: Applying Ecological Theories to Predict Metal Sorption onto Plastic Debris"
author: "Heather Richard, Beth Davis"
date: "2022-11-30"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(hillR)
library(reshape2)
library(dplyr)
library(MASS)
#library(actuar)
library(fitdistrplus)

x <- getURL("https://raw.githubusercontent.com/HLR-04614/final-project/main/metalsdata.csv")
data <- read.csv(text = x)
#data<-read.csv('~/Desktop/Heather.Richard//EvoEcoTheory/metalsdata.csv')
head(data)

data<-read.csv('~/Desktop/Heather.Richard//EvoEcoTheory/metalsdata.csv')
head(data)
```

##Project Overview
#What was your question?

#How did you try to approach it?

#What did you find? OR, what obstacles did you encounter?

#What do you think are the next steps?

#What would be your next questions based on the work you have done so far?

#What would be your words of wisdom to someone trying to continue this work?

#What would you like to take away/remember most about the final project experience?


## Setting up the dataframe

In order to explore the data and run analyses as though it were species data, we'll have to take a few extra steps right off the bat. 
  1. We'll need to average the replicates and add a columm for standard deviation (although we may not use this).
  2. We'll need to melt the data using the reshape2 package so that all metals are under the column heading "variable" and the metal amounts are under the heading "value". 
  3. We'll need to create a column of rank abundance for the metals grouped by material and location.
  *optional* 4. We might add a column for the metals that show what metal family (alkali, alkaline-earth, transition, etc.) to see if patterns in metal sorption is related to metal type. 
  
  Once the data is set up, we can visualize the makeshift species abundance distribution relationships, fit a predictive model and use Hill Numbers to look at the changes over time. 
  

```{r setting up the data frame}
##Melt data with reshape2 to create the dataframe we need but make data1 to keep optical density separate in case...

data1<-melt(data, id = c("Number","Sample","Weight","Unbrushed", "Location","Material","Day.deploy","Optical.density","Replicate")) 

data2<-melt(data, id = c("Number","Sample","Weight","Unbrushed", "Location","Material","Day.deploy","Replicate")) 

#group data and average replicates and tack on sd using dplyr

data3 <- data2 %>% group_by(variable, Location, Day.deploy, Material) %>% summarize(mean_replicate=mean(value), stdev=sd(value))

head(data3)

```
#Now create a rank column

```{r}

data4<-data3 %>% arrange(variable, mean_replicate) %>%
    group_by(Location, Material, Day.deploy) %>% 
    mutate(rank = rank(mean_replicate))
head(data4)
```

#Lets see how that looks using code from "Notes on likelihood" from lab 10: https://github.com/eco-evo-thr-2022/10-likelihood/blob/main/likelihood_how-to.Rmd. 

```{r sad-data}

ggplot(data4, aes(rank, mean_replicate, color=Day.deploy)) +
  geom_point() +
  scale_y_log10() +
  scale_colour_gradient() +
  facet_grid(Location~Material) +
  theme_bw()
```

##Cool! I should have done the ranking in decending order?
Interesting similarities in the shape of the line and changes over time for LDPE and PLA. 

The next step is to follow along Andy in his likelihood methods video and maybe invert the functions he tries to fit.

```{r}
plot(sort(data4$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance') 
```
#probably need to subset those. 


```{r}
head(data4)
CMA<-subset(data4, Location=="CM")
EOS<-subset(data4, data4$Location=="RTC")
EOS_PLA<-subset(EOS, EOS$Material=="PLA")
EOS_Glass<-subset(EOS, EOS$Material=="Glass")
EOS_LDPE<-subset(EOS, EOS$Material=="LDPE")
CMA_PLA<-subset(CMA, CMA$Material=="PLA")
CMA_Glass<-subset(CMA, CMA$Material=="Glass")
CMA_LDPE<-subset(CMA, CMA$Material=="LDPE")


par(mfrow=c(2,3))
plot(sort(EOS_LDPE$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "EOS LDPE") 
plot(sort(EOS_PLA$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "EOS PLA") 
plot(sort(EOS_Glass$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "EOS Glass") 
plot(sort(CMA_LDPE$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "CMA LDPE") 
plot(sort(CMA_PLA$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "CMA PLA") 
plot(sort(CMA_Glass$mean_replicate, decreasing = TRUE), log = 'y', 
     xlab = 'Species rank', ylab = 'Abundance', main= "CMA Glass") 

```

##Why figure out the equation to describe abundance distribution?
Andy explains in the liklihood video that different functional forms can be associated with different processes. For example, logseries functions are associated with neutral processes while lognormal functions might have underlying species interactions. Alternatively, if different processes result in the same patterns in abundance, we can't distinguish the biological mechanisms at work based on the data.

So does the logseries equation describe our data well? 
b= beta. <-What is this?
n= species abundance
Species abundance data in the code is 1:50, but that's not quite going to work with our data since our metal "abundance" values are smaller continuous numbers. 
```{r logseries-eq}

lseries <- function(b, n) {
   1/log(1 / (1 - exp(-b))) * exp(-b * n) / n
}

lseries(0.001, 0.0001:100)
```

```{r}
hist(data4$mean_replicate, probability = TRUE)

points(0.0001:100, lseries(0.001, 0.0001:100), col = 'red', type = 'b')
points(0.0001:100, lseries(0.01, 0.0001:100), col = 'orange', type = 'b')
points(0.0001:100, lseries(0.1, 0.0001:100), col = 'blue', type = 'b')
```

##Now we can see what the probability of our entire dataset (data4$mean_replicate) given different beta values
```{r likelihood}
lseries(0.01, data4$mean_replicate) #It's a long list with inf values so this will have inf values too
prod(lseries(0.01, data4$mean_replicate))
prod(lseries(0.5, data4$mean_replicate))  
```


#Probably this histogram isn't ideal- might need to play with scale/bins
Lets see what happens when we play with log scale and inversion.
We can log transform our data but I don't know how to log transform our equations too. But we can just plot a set of gree points that are normally distributed to see how that looks.

```{r}

hist(log(data4$mean_replicate), probability = TRUE)

points(0.0001:100, lseries(0.001, 0.0001:100), col = 'red', type = 'b')
points(0.0001:100, lseries(0.01, 0.0001:100), col = 'orange', type = 'b')
points(0.0001:100, lseries(0.1, 0.0001:100), col = 'blue', type = 'b')

x <- seq(-10, 5, by = .1)
y <- dnorm(x, mean = -2.0, sd = 3.0)
points(x,y, col="green")

```
##Have to log transform equation##
Just want to play with the data for a minute. If I multiply the metal amounts by 1000 maybe I'll be working with numbers that don't cause issues? No, because 0*1000 is still 0. What if I add a tiny number? 
This is messy. Here is some discussion on this:https://www.researchgate.net/post/Log_transformation_of_values_that_include_0_zero_for_statistical_analyses2
will bookmark this and think of using gamma-, Poisson-, and beta- models instead. But for now...

```{r likelihood}
#data4$mean10<-data4$mean_replicate*1000
data4$mean2<-data4$mean_replicate+0.000000001

sum(log(lseries(0.1, data4$mean2)))
sum(log(lseries(0.01, data4$mean2)))
sum(log(lseries(0.001, data4$mean2)))


```
#Nice! That worked
Now we look at the max likelihood to get the max likelihood estimate of our logseries distribution. What's the best beta value?

```{r ll-curve, echo=FALSE}
bb <- seq(0.001, 0.2, length.out = 50)
ll <- sapply(bb, function(b) sum(log(lseries(b, data4$mean2))))
#pdf('fig_ll.pdf', width = 4, height = 4)
#par(bg = 'black', fg = 'white', col.axis = 'white', col.lab = 'white')
plot(bb, ll, xlab = 'b parameter', ylab = 'log likelihood')
#dev.off()

```
#Now we use a function for finding the optimal beta with the maximum log likelihood value

```{r maxloglik}

llLSeries <- function(b, n) {
    sum(log(lseries(b, n)))
}
optimize(llLSeries, interval = c(0, 10), n = data4$mean2, maximum = TRUE)

```
#should look more into the NA/INF warning
Pika isn't thrilled with working with non-intergers, so unless we can figure that out (multiply by 10000?) these next two chunks are going to be messy. Let's see... This error might be workable?
Error in uniroot(fun, .lseriesSolInt(xbar), tol = .Machine$double.eps) : 
f() values at end points not of opposite sign.   https://stackoverflow.com/questions/38961221/uniroot-solution-in-r 
but just stick with data4$mean2 for now.
```{r pika, warning=FALSE}
#devtools::install_github('ajrominger/pika')
library(pika)
min(data4$mean2)
data4$meanpika<-data4$mean2*1e09

s <- sad(data4$mean2, 'lseries')
#s
 
logLik(s)
```
#hm. I turned off the warnings about non-intergers in this chunk. loglik(s) is still infinite. Sigh. 

```{r}
plot(s, ptype = 'rad') #how can I do this in ggplot so I can facet by variables?
logLikZ(s)
#pchisq(???, df = 1, lower.tail = FALSE) cant do this because logLikZ(s) is infinite
```

```{r model-comparison, warning=FALSE}

ls <- sad(data3$mean2, 'lseries')
ln <- sad(data3$mean2, 'plnorm')
nb <- sad(data3$mean2, 'tnegb')

logLik(ls)
logLik(ln)
logLik(nb)

```
#larger value means data are more probable. I think that means the ln model is better because the nb model is a very large negative number and the ln is a very small negative number? But the ls is infinite...??

```{r}
AIC(ls)
AIC(ln)
AIC(nb)

#IF AIC is smaller by at least 2, it's substantially better
```
#Ok so the ln is better.

```{r}
par(mfrow=c(1,3))
plot(ls, ptype = 'rad', log = 'y', main = 'logseries')
plot(ln, ptype = 'rad', log = 'y', main = 'lognormal')
plot(nb, ptype = 'rad', log = 'y', main = 'negbinom')
```
#look at those plots. I don't know if I buy it.
look at goodness of fit with logLikZ
```{r}
logLikZ(ls)
logLikZ(ln)
logLikZ(nb)
```

##We're going to have to figure out how to deal with our non-interger numbers maybe, but none of these z values are very good- they are pretty huge which makes sense.
We should try other distributions (Poisson, gamma, etc.)

We'll use the MASS package 'fitdistr' using this tutorial: http://www.di.fc.ul.pt/~jpn/r/distributions/fitting.html

```{r}
?fitdistr
```


```{r fitting other distributions, warning=FALSE}
#playing with different data manipulations
dist<-log(data4$mean_replicate)
dist2<-dist[is.infinite(dist)] <- NA
dist3<- na.omit(dist2) 
dist3<-as.numeric(dist3)
dist4<-log(data4$mean2)

fit <- fitdistr(data4$mean2, densfun="normal")  
fit

fit1 <- fitdistr(data4$mean2, densfun="lognormal")  
fit1

fit2 <- fitdistr(data4$mean2, densfun="gamma")
fit2

fit3 <- fitdistr(data4$mean2, densfun="poisson")  
fit3

fit4 <- fitdistr(data4$mean2, densfun="weibull")
fit4
```

```{r}
hist(data4$mean2, pch=20, breaks=25, prob=TRUE, main="")
curve(dnorm(x, fit$estimate[1], fit$estimate[2]), col="red", lwd=2, add=T)
curve(dnorm(x, fit1$estimate[1], fit1$estimate[2]), col="blue", lwd=2, add=T)
curve(dnorm(x, fit2$estimate[1], fit2$estimate[2]), col="yellow", lwd=2, add=T)
curve(dnorm(x, fit3$estimate[1], fit3$estimate[2]), col="orange", lwd=2, add=T)
curve(dnorm(x, fit4$estimate[1], fit4$estimate[2]), col="green", lwd=2, add=T)
```

##We know the data isn't normal, so that makes sense. Looks like log normal and weibull are the closest?
Estimate log likelihood:
 c(0,1) are just initial guesses
```{r}
log_likelihood <- function(params) { -sum(dnorm(data4$mean2, params[1], params[2], log=TRUE)) }
fitll <- optim(c(0,1), log_likelihood)    
fitll
```
#OK but what does it mean?

##Fitdistrplus is awesome!!
```{r}
library(fitdistrplus)

plotdist(data4$mean2, histo = TRUE, demp = TRUE)
```
## the descdist function
shows cool descriptive stats

```{r}
#dist2 <- dist[!is.infinite(dist),]
#dist3<- na.omit(dist) 
#dist3<-as.numeric(dist3)
descdist(data4$mean2, discrete=FALSE, boot=500)
```

#Try the distributions using the library fitdistrplus
```{r}
#dist_pos<-dist4+21

fit_w  <- fitdist(data4$mean2, "weibull")
fit_ln <- fitdist(data4$mean2, "lnorm")
fit_g  <- fitdist(data4$mean2, "gamma")
#fit_p  <- fitdist(data4$mean2, "Poisson")
summary(fit_w)
summary(fit_ln)
summary(fit_g)
```

```{r}
#par(mfrow=c(2,2))
plot.legend <- c("Weibull", "lognormal", "gamma")
denscomp(list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
cdfcomp (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
qqcomp  (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
ppcomp  (list(fit_w, fit_ln, fit_g), legendtext = plot.legend)
```
That's incredibly cool. I think gamma is the winner!
the low AIC values confirm:
summary(fit_w)
Fitting of the distribution ' weibull ' by maximum likelihood 
Parameters : 
Loglikelihood:  2906.011   AIC:  -5808.021   BIC:  -5798.765 
Correlation matrix:
          shape     scale
shape 1.0000000 0.3066056
scale 0.3066056 1.0000000

summary(fit_ln)
Fitting of the distribution ' lnorm ' by maximum likelihood 
Parameters : 
Loglikelihood:  2844.158   AIC:  -5684.317   BIC:  -5675.061 
Correlation matrix:
             meanlog        sdlog
meanlog 1.000000e+00 1.264609e-08
sdlog   1.264609e-08 1.000000e+00

summary(fit_g)
Fitting of the distribution ' gamma ' by maximum likelihood 
Parameters : 
Loglikelihood:  2934.809   AIC:  -5865.618   BIC:  -5856.362 
Correlation matrix:
          shape      rate
shape 1.0000000 0.2984592
rate  0.2984592 1.0000000

```{r}
par(mfrow=c(1,1))


my_data <- data4$mean2

fit_gam <- fitdist(my_data, "gamma")
cdfcomp(fit_gam, xlogscale = TRUE, ylogscale = TRUE)
```
#Still working off of the tutorial http://www.di.fc.ul.pt/~jpn/r/distributions/fitting.html
```{r}
##The code below gives error code 100
#library(actuar)

#fit_ll <- fitdist(my_data, "llogis", start = list(shape = 1, scale = 500)) #gives error code 100
#fit_P  <- fitdist(my_data, "pareto", start = list(shape = 1, scale = 500))
#fit_B  <- fitdist(my_data, "burr",   start = list(shape1 = 0.3, shape2 = 1, rate = 1))
#cdfcomp(list(fit_ln, fit_ll, fit_P), xlogscale = TRUE, ylogscale = TRUE,
 #       legendtext = c("lognormal", "loglogistic", "Pareto"))

cdfcomp(list(fit_ln, fit_w, fit_g), xlogscale = TRUE, ylogscale = TRUE,
        legendtext = c("lognormal", "weibull", "gamma"))
```
#goodness of fit stats
```{r}
gofstat(list(fit_ln, fit_w, fit_g), fitnames = c("lnorm", "weibull", "gamma"))
```
#estimate uncertainty
```{r}
ests <- bootdist(fit_g, niter = 1e3)
summary(ests)
```

```{r}
plot(ests)
```

# 95% percentile bootstrap confidence interval
```{r}
quantile(ests, probs=.05) 
```
In addition to the tutorial we used, this document was really useful (https://cran.r-project.org/web/packages/fitdistrplus/vignettes/paper2JSS.pdf) and provided this insight:

"In ecotoxicology, a lognormal or a loglogistic distribution is often fitted to such a data set in order to characterize the
species sensitivity distribution (SSD) for a pollutant. A low percentile of the fitted distribution, generally the 5%
percentile, is then calculated and named the hazardous concentration 5% (HC5). It is interpreted as the value of
the pollutant concentration protecting 95% of the species (Posthuma et al., 2010)." The above quantile code was made to describe the HC5 of a dataset of the acute toxicity of endosulfan for different species described best by the Burr distribution. 



##Try manual hill numbers from lecture with Renata

```{r}
#head(data4)
ggplot(data4, aes(mean_replicate, variable, color=Day.deploy)) +
  scale_x_log10() +
  geom_point()+
  facet_grid(Location~Material)
```

## From Biodiversity Metrics lecture

equation 3a qD = () p_i is the relative abundance of species (scales sums to 1) similar to song_sad but different. where for each species p_i is the number of plays for that species/song q is the order of the hill number. q cannot be exactly 1 to get Hill numbers as q gets close to 1 use: q\<-1-E-5 ##don't get this part can also use the equation 3b from the hill paper

```{r pressure, echo=TRUE}
head(data4)
metals_sad<-data4$mean_replicate
q<- 2
metals_relabund<- metals_sad/sum(metals_sad)
head(metals_relabund)
sum(metals_relabund)
```

now raise relative abundance of species to the power of q and take the sum of all values\

```{r}
metals_relabund_toq<- metals_relabund^q
head(metals_relabund_toq)
```

```{r}
summed_relabund<-sum(metals_relabund_toq)
summed_relabund_exponent <- summed_relabund ^(1/1-q)
summed_relabund_exponent
```

In some cases it breaks, but for the most part this command below does what we did manually in the steps above

```{r}
hillR::hill_taxa(metals_sad, q)
```


```{r}
#use_git()
```

